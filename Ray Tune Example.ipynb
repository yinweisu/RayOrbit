{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "geological-puzzle",
   "metadata": {},
   "source": [
    "# Ray Tune Example\n",
    "### What is Ray Tune\n",
    "Ray tune is a tool that helps to build distribuited system.\n",
    "\n",
    "It provides implementation of ASHA(https://proceedings.mlsys.org/paper/2020/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf)\n",
    "\n",
    "### Why Integrate Ray into Orbit\n",
    "For machine learning developer, it is very common to do HPO, which requires efficient and easy to use distributed systems. \n",
    "\n",
    "Ray tune and orbit together can provide the developer an end-to-end solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-riding",
   "metadata": {},
   "source": [
    "### Example Setup\n",
    "First let's install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "owned-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-style",
   "metadata": {},
   "source": [
    "Clear previous cluster if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray down -y example-full.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-copper",
   "metadata": {},
   "source": [
    "Lauch up the ray cluster\n",
    "\n",
    "We've modified the namespace to match the current namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accepting-hypothesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mCluster\u001b[39m: \u001b[1mexample-cluster\u001b[22m\n",
      "\n",
      "\u001b[33mLoaded cached provider configuration\u001b[39m\n",
      "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[22m\u001b[26m.\u001b[39m\n",
      "No head node found. Launching a new cluster. \u001b[4mConfirm [y/N]:\u001b[24m y \u001b[2m[automatic, due to --yes]\u001b[22m\n",
      "\n",
      "\u001b[36mAcquiring an up-to-date head node\u001b[39m\n",
      "2021-04-08 16:52:08,677\tINFO node_provider.py:113 -- KubernetesNodeProvider: calling create_namespaced_pod (count=1).\n",
      "  Launched a new head node\n",
      "  \u001b[36mFetching the new head node\u001b[39m\n",
      "  \n",
      "\u001b[2m<1/1>\u001b[22m \u001b[36mSetting up head node\u001b[39m\n",
      "  Prepared bootstrap config\n",
      "  \u001b[37mNew status\u001b[39m: \u001b[1mwaiting-for-ssh\u001b[22m\n",
      "  \u001b[2m[1/7]\u001b[22m \u001b[36mWaiting for SSH to become available\u001b[39m\n",
      "    Running `\u001b[1muptime\u001b[22m\u001b[26m` as a test.\n",
      "2021-04-08 16:52:08,782\tINFO command_runner.py:171 -- NodeUpdater: example-cluster-ray-head-h2mdm: Running kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'\n",
      "error: unable to upgrade connection: container not found (\"ray-node\")\n",
      "    SSH still not available \u001b[2m(Exit Status 1): kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'\u001b[22m\u001b[26m, retrying in \u001b[1m5\u001b[22m\u001b[26m seconds.\n",
      "2021-04-08 16:52:13,889\tINFO command_runner.py:171 -- NodeUpdater: example-cluster-ray-head-h2mdm: Running kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'\n",
      "error: unable to upgrade connection: container not found (\"ray-node\")\n",
      "    SSH still not available \u001b[2m(Exit Status 1): kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'\u001b[22m\u001b[26m, retrying in \u001b[1m5\u001b[22m\u001b[26m seconds.\n",
      "2021-04-08 16:52:19,001\tINFO command_runner.py:171 -- NodeUpdater: example-cluster-ray-head-h2mdm: Running kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'\n",
      "error: unable to upgrade connection: container not found (\"ray-node\")\n",
      "    SSH still not available \u001b[2m(Exit Status 1): kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'\u001b[22m\u001b[26m, retrying in \u001b[1m5\u001b[22m\u001b[26m seconds.\n",
      "2021-04-08 16:52:24,108\tINFO command_runner.py:171 -- NodeUpdater: example-cluster-ray-head-h2mdm: Running kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'\n",
      "error: unable to upgrade connection: container not found (\"ray-node\")\n",
      "    SSH still not available \u001b[2m(Exit Status 1): kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'\u001b[22m\u001b[26m, retrying in \u001b[1m5\u001b[22m\u001b[26m seconds.\n",
      "2021-04-08 16:52:29,214\tINFO command_runner.py:171 -- NodeUpdater: example-cluster-ray-head-h2mdm: Running kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'\n",
      "error: unable to upgrade connection: container not found (\"ray-node\")\n",
      "    SSH still not available \u001b[2m(Exit Status 1): kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'\u001b[22m\u001b[26m, retrying in \u001b[1m5\u001b[22m\u001b[26m seconds.\n",
      "2021-04-08 16:52:34,326\tINFO command_runner.py:171 -- NodeUpdater: example-cluster-ray-head-h2mdm: Running kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (uptime)'\n",
      " 09:52:34 up 8 days, 18:35,  0 users,  load average: 1.22, 0.52, 0.45\n",
      "    \u001b[32mSuccess.\u001b[39m\n",
      "  Updating cluster configuration.\u001b[0m\u001b[2m [hash=a5b63b6f20809a7b75c2295404b6edf03eaca33a]\u001b[22m\u001b[0m\n",
      "  \u001b[37mNew status\u001b[39m: \u001b[1msyncing-files\u001b[22m\n",
      "  \u001b[2m[2/7]\u001b[22m \u001b[36mProcessing file mounts\u001b[39m\n",
      "2021-04-08 16:52:34,862\tINFO command_runner.py:171 -- NodeUpdater: example-cluster-ray-head-h2mdm: Running kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (mkdir -p ~)'\n",
      "/opt/conda/lib/python3.8/site-packages/ray/autoscaler/_private/kubernetes/kubectl-rsync.sh: line 9: exec: rsync: not found\n",
      "/opt/conda/lib/python3.8/site-packages/ray/autoscaler/_private/command_runner.py:210: UserWarning: NodeUpdater: example-cluster-ray-head-h2mdm: rsync failed: 'Command '['/opt/conda/lib/python3.8/site-packages/ray/autoscaler/_private/kubernetes/kubectl-rsync.sh', '-aqz', '/tmp/ray-bootstrap-pywbbwgp', 'example-cluster-ray-head-h2mdm@lake-creator:/home/ray/ray_bootstrap_config.yaml']' returned non-zero exit status 127.'. Falling back to 'kubectl cp'\n",
      "  warnings.warn(\n",
      "  \u001b[2m[3/7]\u001b[22m No worker file mounts to sync\n",
      "  \u001b[37mNew status\u001b[39m: \u001b[1msetting-up\u001b[22m\n",
      "  \u001b[2m[4/7]\u001b[22m No initialization commands to run.\n",
      "  \u001b[2m[5/7]\u001b[22m \u001b[36mInitalizing command runner\u001b[39m\n",
      "  \u001b[2m[6/7]\u001b[22m No setup commands to run.\n",
      "  \u001b[2m[7/7]\u001b[22m \u001b[36mStarting the Ray runtime\u001b[39m\n",
      "2021-04-08 16:52:35,838\tINFO command_runner.py:171 -- NodeUpdater: example-cluster-ray-head-h2mdm: Running kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (export RAY_OVERRIDE_RESOURCES='\"'\"'{\"CPU\":1,\"GPU\":0}'\"'\"';ray stop)'\n",
      "Did not find any active Ray processes.\n",
      "\u001b[0m2021-04-08 16:52:36,764\tINFO command_runner.py:171 -- NodeUpdater: example-cluster-ray-head-h2mdm: Running kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash --login -c -i 'true && source ~/.bashrc && export OMP_NUM_THREADS=1 PYTHONWARNINGS=ignore && (export RAY_OVERRIDE_RESOURCES='\"'\"'{\"CPU\":1,\"GPU\":0}'\"'\"';ulimit -n 65536; ray start --head --autoscaling-config=~/ray_bootstrap_config.yaml --dashboard-host 0.0.0.0)'\n",
      "\u001b[37mLocal node IP\u001b[39m: \u001b[1m10.0.23.142\u001b[22m\n",
      "2021-04-08 09:52:38,583\tINFO services.py:1264 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://0.0.0.0:8265\u001b[39m\u001b[22m\n",
      "\n",
      "\u001b[32m--------------------\u001b[39m\n",
      "\u001b[32mRay runtime started.\u001b[39m\n",
      "\u001b[32m--------------------\u001b[39m\n",
      "\n",
      "\u001b[36mNext steps\u001b[39m\n",
      "  To connect to this Ray runtime from another node, run\n",
      "  \u001b[1m  ray start --address='10.0.23.142:6379' --redis-password='5241590000000000'\u001b[22m\n",
      "  \n",
      "  Alternatively, use the following Python code:\n",
      "    \u001b[31mimport\u001b[39m\u001b[26m ray\n",
      "    ray\u001b[31m.\u001b[39m\u001b[26minit(address\u001b[31m=\u001b[39m\u001b[26m\u001b[33m'auto'\u001b[39m\u001b[26m, _redis_password\u001b[31m=\u001b[39m\u001b[26m\u001b[33m'5241590000000000'\u001b[39m\u001b[26m)\n",
      "  \n",
      "  \u001b[4mIf connection fails, check your firewall settings and network configuration.\u001b[24m\n",
      "  \n",
      "  To terminate the Ray runtime, run\n",
      "  \u001b[1m  ray stop\u001b[22m\n",
      "\u001b[0m  \u001b[37mNew status\u001b[39m: \u001b[1mup-to-date\u001b[22m\n",
      "\n",
      "\u001b[36mUseful commands\u001b[39m\n",
      "  Monitor autoscaling with\n",
      "  \u001b[1m  ray exec /efs/weisy/example-full.yaml 'tail -n 100 -f /tmp/ray/session_latest/logs/monitor*'\u001b[22m\n",
      "  Connect to a terminal on the cluster head:\n",
      "  \u001b[1m  ray attach /efs/weisy/example-full.yaml\u001b[22m\n",
      "  Get a remote shell to the cluster manually:\n",
      "    kubectl -n lake-creator exec -it example-cluster-ray-head-h2mdm -- bash\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!ray up -y example-full.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-mission",
   "metadata": {},
   "source": [
    "Check the cluster got launched by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intellectual-prime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                     READY   STATUS      RESTARTS   AGE\n",
      "efs-init-lake-creator-r8sht              0/1     Completed   0          8d\n",
      "example-cluster-ray-head-h2mdm           1/1     Running     0          38s\n",
      "jupyter-weisy                            1/1     Running     0          6m31s\n",
      "jupyterhub-5fd6b56b94-59nhs              1/1     Running     0          8d\n",
      "ray-test-job-4mtx5-8p8dq                 0/1     Completed   0          20h\n",
      "ray-test-job-9svlb-nrjgz                 0/1     Completed   0          19h\n",
      "ray-test-job-hqdt9-qhtm4                 0/1     Completed   0          19h\n",
      "ray-test-job-rlh5n-pwbbt                 0/1     Completed   0          19h\n",
      "ray-test-job-srxwf-kswwj                 0/1     Completed   0          19h\n",
      "ray-test-job-vdfcv-cdgbn                 0/1     Completed   0          19h\n",
      "ray-test-job-zxvsb-28lm2                 0/1     Completed   0          19h\n",
      "team-script-team-script-launcher-4g2cc   0/1     Pending     0          8d\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-commons",
   "metadata": {},
   "source": [
    "To create a sample job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blond-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job.batch/ray-test-job-46d7c created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create -f job-example.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accomplished-manchester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                     READY   STATUS              RESTARTS   AGE\n",
      "efs-init-lake-creator-r8sht              0/1     Completed           0          8d\n",
      "example-cluster-ray-head-h2mdm           1/1     Running             0          45s\n",
      "jupyter-weisy                            1/1     Running             0          6m38s\n",
      "jupyterhub-5fd6b56b94-59nhs              1/1     Running             0          8d\n",
      "ray-test-job-46d7c-bk55t                 0/1     ContainerCreating   0          4s\n",
      "ray-test-job-4mtx5-8p8dq                 0/1     Completed           0          20h\n",
      "ray-test-job-9svlb-nrjgz                 0/1     Completed           0          19h\n",
      "ray-test-job-hqdt9-qhtm4                 0/1     Completed           0          19h\n",
      "ray-test-job-rlh5n-pwbbt                 0/1     Completed           0          19h\n",
      "ray-test-job-srxwf-kswwj                 0/1     Completed           0          19h\n",
      "ray-test-job-vdfcv-cdgbn                 0/1     Completed           0          19h\n",
      "ray-test-job-zxvsb-28lm2                 0/1     Completed           0          19h\n",
      "team-script-team-script-launcher-4g2cc   0/1     Pending             0          8d\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-collector",
   "metadata": {},
   "source": [
    "Find the pod we just created and log it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "described-municipality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-08 09:52:54--  https://gist.githubusercontent.com/yinweisu/39e18a73105f0190e0e5f057273d60b5/raw/e738771f206eb226b1204fdc9431e7ee535a1e1d/job_example.py\n",
      "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 798 [text/plain]\n",
      "Saving to: ‘job_example.py’\n",
      "\n",
      "     0K                                                       100% 71.2M=0s\n",
      "\n",
      "2021-04-08 09:52:55 (71.2 MB/s) - ‘job_example.py’ saved [798/798]\n",
      "\n",
      "2021-04-08 09:52:56,508\tINFO services.py:1264 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-04-08 09:52:56,510\tWARNING services.py:1717 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2021-04-08 09:52:57,846\tWARNING function_runner.py:545 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "2021-04-08 09:52:57,939\tWARNING tune.py:494 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n",
      "2021-04-08 09:52:59,449\tINFO tune.py:549 -- Total run time: 1.61 seconds (1.40 seconds for the tuning loop).\n",
      "== Status ==\n",
      "Memory usage on this node: 5.3/186.8 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/1 CPUs, 0/4 GPUs, 0.0/168.13 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /home/ray/ray_results/training_function_2021-04-08_09-52-57\n",
      "Number of trials: 3/3 (2 PENDING, 1 RUNNING)\n",
      "+-------------------------------+----------+-------+---------+--------+\n",
      "| Trial name                    | status   | loc   |   alpha |   beta |\n",
      "|-------------------------------+----------+-------+---------+--------|\n",
      "| training_function_def69_00000 | RUNNING  |       |   0.001 |      1 |\n",
      "| training_function_def69_00001 | PENDING  |       |   0.01  |      1 |\n",
      "| training_function_def69_00002 | PENDING  |       |   0.1   |      3 |\n",
      "+-------------------------------+----------+-------+---------+--------+\n",
      "\n",
      "\n",
      "Result for training_function_def69_00000:\n",
      "  date: 2021-04-08_09-52-58\n",
      "  done: false\n",
      "  experiment_id: ec643c0818974ccd8369397002e54466\n",
      "  hostname: ray-test-job-46d7c-bk55t\n",
      "  iterations_since_restore: 1\n",
      "  mean_loss: 10.1\n",
      "  neg_mean_loss: -10.1\n",
      "  node_ip: 10.0.20.105\n",
      "  pid: 122\n",
      "  time_since_restore: 0.0001964569091796875\n",
      "  time_this_iter_s: 0.0001964569091796875\n",
      "  time_total_s: 0.0001964569091796875\n",
      "  timestamp: 1617900778\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: def69_00000\n",
      "  \n",
      "Result for training_function_def69_00000:\n",
      "  date: 2021-04-08_09-52-58\n",
      "  done: true\n",
      "  experiment_id: ec643c0818974ccd8369397002e54466\n",
      "  experiment_tag: 0_alpha=0.001,beta=1\n",
      "  hostname: ray-test-job-46d7c-bk55t\n",
      "  iterations_since_restore: 10\n",
      "  mean_loss: 10.091008092716553\n",
      "  neg_mean_loss: -10.091008092716553\n",
      "  node_ip: 10.0.20.105\n",
      "  pid: 122\n",
      "  time_since_restore: 0.022188663482666016\n",
      "  time_this_iter_s: 0.002338409423828125\n",
      "  time_total_s: 0.022188663482666016\n",
      "  timestamp: 1617900778\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: def69_00000\n",
      "  \n",
      "Result for training_function_def69_00001:\n",
      "  date: 2021-04-08_09-52-58\n",
      "  done: false\n",
      "  experiment_id: 1b27ccd09e60416aa8d5d38c36f0397a\n",
      "  hostname: ray-test-job-46d7c-bk55t\n",
      "  iterations_since_restore: 1\n",
      "  mean_loss: 10.1\n",
      "  neg_mean_loss: -10.1\n",
      "  node_ip: 10.0.20.105\n",
      "  pid: 177\n",
      "  time_since_restore: 0.00013136863708496094\n",
      "  time_this_iter_s: 0.00013136863708496094\n",
      "  time_total_s: 0.00013136863708496094\n",
      "  timestamp: 1617900778\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: def69_00001\n",
      "  \n",
      "Result for training_function_def69_00001:\n",
      "  date: 2021-04-08_09-52-58\n",
      "  done: true\n",
      "  experiment_id: 1b27ccd09e60416aa8d5d38c36f0397a\n",
      "  experiment_tag: 1_alpha=0.01,beta=1\n",
      "  hostname: ray-test-job-46d7c-bk55t\n",
      "  iterations_since_restore: 10\n",
      "  mean_loss: 10.010802775024777\n",
      "  neg_mean_loss: -10.010802775024777\n",
      "  node_ip: 10.0.20.105\n",
      "  pid: 177\n",
      "  time_since_restore: 0.026111364364624023\n",
      "  time_this_iter_s: 0.0025136470794677734\n",
      "  time_total_s: 0.026111364364624023\n",
      "  timestamp: 1617900778\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: def69_00001\n",
      "  \n",
      "Result for training_function_def69_00002:\n",
      "  date: 2021-04-08_09-52-59\n",
      "  done: false\n",
      "  experiment_id: 15af594d204e40d4a5cfdc3527ec875f\n",
      "  hostname: ray-test-job-46d7c-bk55t\n",
      "  iterations_since_restore: 1\n",
      "  mean_loss: 10.3\n",
      "  neg_mean_loss: -10.3\n",
      "  node_ip: 10.0.20.105\n",
      "  pid: 203\n",
      "  time_since_restore: 0.0001163482666015625\n",
      "  time_this_iter_s: 0.0001163482666015625\n",
      "  time_total_s: 0.0001163482666015625\n",
      "  timestamp: 1617900779\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: def69_00002\n",
      "  \n",
      "Result for training_function_def69_00002:\n",
      "  date: 2021-04-08_09-52-59\n",
      "  done: true\n",
      "  experiment_id: 15af594d204e40d4a5cfdc3527ec875f\n",
      "  experiment_tag: 2_alpha=0.1,beta=3\n",
      "  hostname: ray-test-job-46d7c-bk55t\n",
      "  iterations_since_restore: 10\n",
      "  mean_loss: 9.474311926605504\n",
      "  neg_mean_loss: -9.474311926605504\n",
      "  node_ip: 10.0.20.105\n",
      "  pid: 203\n",
      "  time_since_restore: 0.018704652786254883\n",
      "  time_this_iter_s: 0.0019659996032714844\n",
      "  time_total_s: 0.018704652786254883\n",
      "  timestamp: 1617900779\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: def69_00002\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 5.3/186.8 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/1 CPUs, 0/4 GPUs, 0.0/168.13 GiB heap, 0.0/9.31 GiB objects (0.0/1.0 accelerator_type:T4)\n",
      "Result logdir: /home/ray/ray_results/training_function_2021-04-08_09-52-57\n",
      "Number of trials: 3/3 (3 TERMINATED)\n",
      "+-------------------------------+------------+-------+---------+--------+----------+--------+------------------+-----------------+\n",
      "| Trial name                    | status     | loc   |   alpha |   beta |     loss |   iter |   total time (s) |   neg_mean_loss |\n",
      "|-------------------------------+------------+-------+---------+--------+----------+--------+------------------+-----------------|\n",
      "| training_function_def69_00000 | TERMINATED |       |   0.001 |      1 | 10.091   |     10 |        0.0221887 |       -10.091   |\n",
      "| training_function_def69_00001 | TERMINATED |       |   0.01  |      1 | 10.0108  |     10 |        0.0261114 |       -10.0108  |\n",
      "| training_function_def69_00002 | TERMINATED |       |   0.1   |      3 |  9.47431 |     10 |        0.0187047 |        -9.47431 |\n",
      "+-------------------------------+------------+-------+---------+--------+----------+--------+------------------+-----------------+\n",
      "\n",
      "\n",
      "Best config:  {'alpha': 0.1, 'beta': 3}\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs ray-test-job-46d7c-bk55t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-quilt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
